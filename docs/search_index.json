[
["index.html", "R for FinTech Chapter 1 Introduction 1.1 Welcome 1.2 The purpose of this book 1.3 How this book is organized 1.4 Prerequisites", " R for FinTech Jasmine Dumas 2016-10-09 Chapter 1 Introduction 1.1 Welcome Welcome to R for FinTech! This guidebook has emphasis on “FinTech” or Financial Technology applications in data analysis. Examples and packages in this guidebook will highlight common methods in computational programming for banking, insurance, and investing. 1.2 The purpose of this book When starting out in a new industry or a new programming language like R, it can be difficult to learn how to apply industry-specific methods given the vast amount of R packages available and the sparsity of relative examples using financial data on question and answer forums. The purpose of this book is to provide introductory resources and modular code examples to enable the effective communication and translation of financial data to actionable-insights. 1.3 How this book is organized The organization of this guidebook is inspired by the book R for Data Science from Garrett Grolemund and Hadley Wickham which explores each step of the data science process from acquiring data on the web to communicating the outputs with dynamic reports and dashboards. Each section of the guidebook is meant to follow the typical data science workflow when followed in order however when jumping into existing projects which is a common approach in industry, the sections can be referenced as needed as standalone tutorials. 1.4 Prerequisites If you don’t already have R or RStudio: Download R at https://www.r-project.org/alt-home/ Download RStudio at https://www.rstudio.com/products/rstudio/download/ "],
["import.html", "Chapter 2 Import 2.1 Tabular Data 2.2 Hierarchical Data 2.3 Relational Data 2.4 Distributed Data 2.5 Different Data Formats 2.6 Accessing Zipped Data files", " Chapter 2 Import The first step in the typical data science project involves importing data into R. There are numerous packages for different data types all with varying preferences on speed and efficiency. Here are some R packages for importing data into R: 2.1 Tabular Data Tabular data consists of variables, observations and values to form data frames. This is the most common format of organized data and many packages are developed to work with this type of data. readr: Read flat/tabular text files from disk (or a connection). readr has some benefits over the base/utils version as smart column type parsing and not automatically converting strings into factors. Here is an example of credit card applications from the UCI Machine Learning Repository: library(readr) cc_apps &lt;- read_csv(&quot;http://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data&quot;, col_names = F) head(cc_apps) ## # A tibble: 6 × 16 ## X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 X11 X12 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 b 30.83 0.000 u g w v 1.25 t t 01 f ## 2 a 58.67 4.460 u g q h 3.04 t t 06 f ## 3 a 24.50 0.500 u g q h 1.50 t f 0 f ## 4 b 27.83 1.540 u g w v 3.75 t t 05 t ## 5 b 20.17 5.625 u g w v 1.71 t f 0 f ## 6 b 32.08 4.000 u g m v 2.50 t f 0 t ## # ... with 4 more variables: X13 &lt;chr&gt;, X14 &lt;chr&gt;, X15 &lt;int&gt;, X16 &lt;chr&gt; readxl: Import excel files into R. Supports ‘.xls’ via the embedded ‘libxls’ C library (http://sourceforge.net/projects/libxls/) and ‘.xlsx’ via the embedded ‘RapidXML’ C++ library (http://rapidxml.sourceforge.net). Works on Windows, Mac and Linux without external dependencies. Here is an example from the default of credit card clients data set from the UCI Machine Learning Repository: # download the excel file first from the link library(readxl) default_cc &lt;- read_excel(&quot;default of credit card clients.xls&quot;) # alternative reading from a URL require(RCurl) require(gdata) url &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls&quot; default_cc &lt;- read.xls(url) head(default_cc) ## X X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 ## 1 ID LIMIT_BAL SEX EDUCATION MARRIAGE AGE PAY_0 PAY_2 PAY_3 PAY_4 PAY_5 ## 2 1 20000 2 2 1 24 2 2 -1 -1 -2 ## 3 2 120000 2 2 2 26 -1 2 0 0 0 ## 4 3 90000 2 2 2 34 0 0 0 0 0 ## 5 4 50000 2 2 1 37 0 0 0 0 0 ## 6 5 50000 1 2 1 57 -1 0 -1 0 0 ## X11 X12 X13 X14 X15 X16 X17 ## 1 PAY_6 BILL_AMT1 BILL_AMT2 BILL_AMT3 BILL_AMT4 BILL_AMT5 BILL_AMT6 ## 2 -2 3913 3102 689 0 0 0 ## 3 2 2682 1725 2682 3272 3455 3261 ## 4 0 29239 14027 13559 14331 14948 15549 ## 5 0 46990 48233 49291 28314 28959 29547 ## 6 0 8617 5670 35835 20940 19146 19131 ## X18 X19 X20 X21 X22 X23 ## 1 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 ## 2 0 689 0 0 0 0 ## 3 0 1000 1000 1000 0 2000 ## 4 1518 1500 1000 1000 1000 5000 ## 5 2000 2019 1200 1100 1069 1000 ## 6 2000 36681 10000 9000 689 679 ## Y ## 1 default payment next month ## 2 1 ## 3 1 ## 4 0 ## 5 0 ## 6 0 2.2 Hierarchical Data Hierarchical Data is a tree-structure data format such as XML, HTML, JSON. jsonlite: A fast JSON parser and generator optimized for statistical data and the web. xml2: Work with XML files using a simple, consistent interface. Built on top of the ‘libxml2’ C library. rvest: Wrappers around the ‘xml2’ and ‘httr’ packages to make it easy to download, then manipulate, HTML and XML. 2.3 Relational Data Relational Data consists of a collection of data items (tables) organized as a set based on the data contents and its relation. DBI: A database interface definition for communication between R and relational database management systems. All classes in this package are virtual and need to be extended by the various R/DBMS implementations. RMySQL: Implements ‘DBI’ Interface to ‘MySQL’ and ‘MariaDB’ Databases. 2.4 Distributed Data Distributed Data consists of non-relational formats with quick access to data over a large number of nodes. sparklyr: Filter and aggregate Spark datasets then bring them into R for analysis and visualization. 2.5 Different Data Formats The R programming language and environment is continuously increasing its capacity with new packages to work with different types of proprietory data formats from statistical software packages that are used on industry teams. haven: Import and Export ‘SPSS’, ‘Stata’ and ‘SAS’ Files. Here is an example from Macquarie University data repository for the applied finance and actuarial studies of importing a SAS data set: library(haven) claims &lt;- read_sas(&quot;http://www.businessandeconomics.mq.edu.au/our_departments/Applied_Finance_and_Actuarial_Studies/acst_docs/glms_for_insurance_data/data/claims_sas_miner.sas7bdat&quot;) head(claims) ## # A tibble: 6 × 33 ## ID KIDSDRIV PLCYDATE TRAVTIME CAR_USE POLICYNO BLUEBOOK ## &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 100058542 0 1996-03-17 17.09181 Private 36292520 9860 ## 2 100093408 0 1993-07-26 17.98656 Private 31958061 1500 ## 3 100208113 0 1994-06-06 47.00727 Commercial 42433312 30460 ## 4 100237269 0 1999-01-19 31.24381 Private 49896544 16580 ## 5 10042968 0 1999-05-18 13.96243 Commercial 79298192 23030 ## 6 100737644 0 1996-02-28 45.79204 Private 43393435 20730 ## # ... with 26 more variables: INITDATE &lt;date&gt;, RETAINED &lt;dbl&gt;, ## # NPOLICY &lt;dbl&gt;, CAR_TYPE &lt;chr&gt;, RED_CAR &lt;chr&gt;, OLDCLAIM &lt;dbl&gt;, ## # CLM_FREQ &lt;dbl&gt;, REVOLKED &lt;chr&gt;, MVR_PTS &lt;dbl&gt;, CLM_AMT &lt;dbl&gt;, ## # CLM_DATE &lt;date&gt;, CLM_FLAG &lt;chr&gt;, BIRTH &lt;date&gt;, AGE &lt;dbl&gt;, ## # HOMEKIDS &lt;dbl&gt;, YOJ &lt;dbl&gt;, INCOME &lt;dbl&gt;, GENDER &lt;chr&gt;, MARRIED &lt;chr&gt;, ## # PARENT1 &lt;chr&gt;, JOBCLASS &lt;chr&gt;, MAX_EDUC &lt;chr&gt;, HOME_VAL &lt;dbl&gt;, ## # SAMEHOME &lt;dbl&gt;, DENSITY &lt;chr&gt;, YEARQTR &lt;chr&gt; foreign: Functions for reading and writing data stored by some versions of Epi Info, Minitab, S, SAS, SPSS, Stata, Systat and Weka and for reading and writing some dBase files. 2.6 Accessing Zipped Data files Zip archives are actually more a ‘filesystem’ with content, meta data, and/or documentation. Create a temp file. file name (eg tempfile()) Use download.file() to download the file into the temp object that is being reserved for the file Use unzip() to extract the target file from temp file by reading the meta data on what specific data set you want which is contained in the zip file Remove the temp file via unlink() temp &lt;- tempfile() download.file(&quot;http://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank.zip&quot;,temp) unzip(temp, &quot;bank.csv&quot;) bank_marketing &lt;- read.csv(&quot;bank.csv&quot;, sep=&quot;;&quot;) # sometimes its the default unlink(temp) head(bank_marketing) ## age job marital education default balance housing loan contact ## 1 30 unemployed married primary no 1787 no no cellular ## 2 33 services married secondary no 4789 yes yes cellular ## 3 35 management single tertiary no 1350 yes no cellular ## 4 30 management married tertiary no 1476 yes yes unknown ## 5 59 blue-collar married secondary no 0 yes no unknown ## 6 35 management single tertiary no 747 no no cellular ## day month duration campaign pdays previous poutcome y ## 1 19 oct 79 1 -1 0 unknown no ## 2 11 may 220 1 339 4 failure no ## 3 16 apr 185 1 330 1 failure no ## 4 3 jun 199 4 -1 0 unknown no ## 5 5 may 226 1 -1 0 unknown no ## 6 23 feb 141 2 176 3 failure no "],
["tidy.html", "Chapter 3 Tidy", " Chapter 3 Tidy Reshaping the data is an important (if necessary) step to exploratory data analysis and preparatory data cleaning for modeling or creating specialized visualization. Further concepts about tidy data can be found in this paper Tidy Data. The principles of tidy data are: Each variable forms a column Each observation forms a row Each type of observational unit forms a table tidyr: An evolution of ‘reshape2’. It’s designed specifically for data tidying (not general reshaping or aggregating) and works well with ‘dplyr’ data pipelines. Tidy data complements R’s vectorized operations. R will automatically preserve observations as you manipulate variables. No other format works as intuitively with R. Examples: library(tidyr) library(insuranceData) library(magrittr) data(&quot;AutoBi&quot;) head(AutoBi) ## CASENUM ATTORNEY CLMSEX MARITAL CLMINSUR SEATBELT CLMAGE LOSS ## 1 5 1 1 NA 2 1 50 34.940 ## 2 13 2 2 2 1 1 28 10.892 ## 3 66 2 1 2 2 1 5 0.330 ## 4 71 1 1 1 2 2 32 11.037 ## 5 96 2 1 4 2 1 30 0.138 ## 6 97 1 2 1 2 1 35 0.309 # create an interaction column g &lt;- AutoBi %&gt;% unite(col = MARITAL_CLMAGE, MARITAL, CLMAGE, sep = &quot;*&quot;) head(g) ## CASENUM ATTORNEY CLMSEX MARITAL_CLMAGE CLMINSUR SEATBELT LOSS ## 1 5 1 1 NA*50 2 1 34.940 ## 2 13 2 2 2*28 1 1 10.892 ## 3 66 2 1 2*5 2 1 0.330 ## 4 71 1 1 1*32 2 2 11.037 ## 5 96 2 1 4*30 2 1 0.138 ## 6 97 1 2 1*35 2 1 0.309 Examples: Not all data that needs to be tidied comes in “long” format (i.e. the spread() function), so this data set below is put into tidy form using base R functions. This data set has its observations stored in the row names field, delimited with a semicolon “;”. The original column contains a mis-transformed value for the population density metric. The only variables’ names are also a concatenation of the entire data set’s column names delimited with a period “.”. Data dictionary Original source data used in the insuranceData package library(insuranceData) library(magrittr) data(&quot;Thirdparty&quot;) head(Thirdparty) # rownames have column values ## lga.sd.claims.accidents.ki.population.pop_density ## ASHFIELD;1;1103;2304;920;124850;0 499001 ## AUBURN;1;1939;2660;1465;143500;0 148379 ## BANKSTOWN;1;4339;7381;3864;470700;0 205407 ## BAULKHAMHILLS;1;1491;3217;1554;311300;0 25879 ## BLACKTOWN;1;3801;6655;4175;584900;0 81222 ## BOTANY;1;387;2013;854;106350;0 178143 cols = strsplit(colnames(Thirdparty) , &quot;.&quot; , fixed=T) # a new vector to use later dput(unlist(cols)) ## c(&quot;lga&quot;, &quot;sd&quot;, &quot;claims&quot;, &quot;accidents&quot;, &quot;ki&quot;, &quot;population&quot;, &quot;pop_density&quot; ## ) rows2df &lt;- sapply(rownames(Thirdparty), strsplit, &quot;;&quot;, USE.NAMES = FALSE) tidy_3PD &lt;- data.frame(matrix(unlist(rows2df), nrow = nrow(Thirdparty), byrow=T)) colnames(tidy_3PD) &lt;- c(&quot;lga&quot;, &quot;sd&quot;, &quot;claims&quot;, &quot;accidents&quot;, &quot;ki&quot;, &quot;population&quot;, &quot;pop_density&quot;) tidy_3PD$pop_density &lt;- Thirdparty$lga.sd.claims.accidents.ki.population.pop_density / 1000000 head(tidy_3PD) ## lga sd claims accidents ki population pop_density ## 1 ASHFIELD 1 1103 2304 920 124850 0.499001 ## 2 AUBURN 1 1939 2660 1465 143500 0.148379 ## 3 BANKSTOWN 1 4339 7381 3864 470700 0.205407 ## 4 BAULKHAMHILLS 1 1491 3217 1554 311300 0.025879 ## 5 BLACKTOWN 1 3801 6655 4175 584900 0.081222 ## 6 BOTANY 1 387 2013 854 106350 0.178143 Every variable forms a column and every observation forms a row which makes for a table! "],
["transform.html", "Chapter 4 Transform", " Chapter 4 Transform Transforming cleaned data to create summaries is an important part of the data analysis process along with extracting out features to create new features in variable. dplyr: A fast, consistent tool for working with data frame like objects, both in memory and out of memory. Examples: library(dplyr) library(insuranceData) library(magrittr) data(&quot;AutoBi&quot;) # summary of loss by whether or not the claimant was wearing a seatbelt/child restraint AutoBi %&gt;% group_by(SEATBELT, MARITAL) %&gt;% summarise(mLOSS = median(LOSS)) ## Source: local data frame [10 x 3] ## Groups: SEATBELT [?] ## ## SEATBELT MARITAL mLOSS ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 1 2.641 ## 2 1 2 1.780 ## 3 1 3 1.703 ## 4 1 4 3.845 ## 5 1 NA 3.120 ## 6 2 1 3.919 ## 7 2 2 2.328 ## 8 NA 1 1.985 ## 9 NA 2 1.433 ## 10 NA NA 2.364 "],
["viz.html", "Chapter 5 Visualization 5.1 Exploratory Visualization 5.2 Interactive Visualization 5.3 Other visualization packages", " Chapter 5 Visualization Data Visualization allows for the effective translation of complex models and processes to business applicable decisions. 5.1 Exploratory Visualization Exploratory visualization invloves learning descriptive details prior to modeling efforts. Preemptive results from visualizing distributions can lead to more informed approachs in for example parameter tunning. ggplot2: An implementation of the grammar of graphics in R. It combines the advantages of both base and lattice graphics: conditioning and shared axes are handled automatically, and you can still build up a plot step by step from multiple data sources. It also implements a sophisticated multidimensional conditioning system and a consistent interface to map data to aesthetic attributes. Examples: library(ggplot2) library(insuranceData) data(&quot;AutoClaims&quot;) head(AutoClaims) ## STATE CLASS GENDER AGE PAID ## 1 STATE 14 C6 M 97 1134.44 ## 2 STATE 15 C6 M 96 3761.24 ## 3 STATE 15 C11 M 95 7842.31 ## 4 STATE 15 F6 F 95 2384.67 ## 5 STATE 15 F6 M 95 650.00 ## 6 STATE 15 F6 M 95 391.12 g &lt;- ggplot(AutoClaims, aes(x = AGE)) + geom_bar() + facet_grid(. ~ GENDER) g _____________ library(ggplot2) library(insuranceData) g2 &lt;- ggplot(AutoClaims, aes(x = AGE, y = PAID, color = GENDER)) + geom_point() + geom_text(aes(label = STATE)) + theme_classic() g2 5.2 Interactive Visualization plotly: Easily translate ‘ggplot2’ graphs to an interactive web-based version and/or create custom web-based visualizations directly from R. Once uploaded to a ‘plotly’ account, ‘plotly’ graphs (and the data behind them) can be viewed and modified in a web browser. Examples: suppressPackageStartupMessages(library(plotly)) library(insuranceData) data(&quot;AutoCollision&quot;) head(AutoCollision) ## Age Vehicle_Use Severity Claim_Count ## 1 A Pleasure 250.48 21 ## 2 A DriveShort 274.78 40 ## 3 A DriveLong 244.52 23 ## 4 A Business 797.80 5 ## 5 B Pleasure 213.71 63 ## 6 B DriveShort 298.60 171 plot_ly(AutoCollision, x = Severity, y = Claim_Count, mode = &quot;markers&quot;, color = Severity, size = Severity) 5.3 Other visualization packages rcharts leaflet ggvis htmlwidget googleVis "],
["model.html", "Chapter 6 Model 6.1 LM 6.2 GLM 6.3 GBM 6.4 Ensemble learning 6.5 Additional Machine Learning Techniques", " Chapter 6 Model Build Models 6.1 LM lm(): In statistics, the term linear model is used for drawing primary associations with a response (dependent variable) and covariate(s) (independent variable(s)) as a regression analysis technique. Source: Wikipedia Examples: library(insuranceData) data(&quot;AutoCollision&quot;) head(AutoCollision) ## Age Vehicle_Use Severity Claim_Count ## 1 A Pleasure 250.48 21 ## 2 A DriveShort 274.78 40 ## 3 A DriveLong 244.52 23 ## 4 A Business 797.80 5 ## 5 B Pleasure 213.71 63 ## 6 B DriveShort 298.60 171 fit &lt;- lm(Severity ~ Vehicle_Use + Age + Claim_Count, data = AutoCollision) summary(fit) ## ## Call: ## lm(formula = Severity ~ Vehicle_Use + Age + Claim_Count, data = AutoCollision) ## ## Residuals: ## Min 1Q Median 3Q Max ## -130.430 -24.580 -1.353 23.368 274.270 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 523.0303 51.1632 10.223 2.18e-09 *** ## Vehicle_UseDriveLong -150.3807 50.6663 -2.968 0.007603 ** ## Vehicle_UseDriveShort -198.6347 65.8048 -3.019 0.006786 ** ## Vehicle_UsePleasure -184.4265 40.9901 -4.499 0.000219 *** ## AgeB -105.7532 58.6628 -1.803 0.086521 . ## AgeC -128.0870 65.4756 -1.956 0.064546 . ## AgeD -137.4725 68.6554 -2.002 0.058992 . ## AgeE -206.6701 70.2024 -2.944 0.008026 ** ## AgeF -195.6402 97.7303 -2.002 0.059052 . ## AgeG -183.3416 85.0540 -2.156 0.043476 * ## AgeH -173.1478 71.6721 -2.416 0.025387 * ## Claim_Count 0.1000 0.1468 0.681 0.503380 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 81.67 on 20 degrees of freedom ## Multiple R-squared: 0.6472, Adjusted R-squared: 0.4532 ## F-statistic: 3.336 on 11 and 20 DF, p-value: 0.009379 # this is not the best model we could have constructed as the lm assumes the error distribution of the response to be normal (gaussian) - and for a severity model we know that a multiplicative Gamma distribution is more appropriate. 6.2 GLM glm(): In statistics, the generalized linear model (GLM) is a flexible generalization of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution. The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value. Source: Wikipedia Examples: library(insuranceData) data(&quot;AutoCollision&quot;) fit &lt;- glm(Severity ~ Vehicle_Use + Age + Claim_Count, data = AutoCollision, family = Gamma(link = &quot;inverse&quot;)) summary(fit) ## ## Call: ## glm(formula = Severity ~ Vehicle_Use + Age + Claim_Count, family = Gamma(link = &quot;inverse&quot;), ## data = AutoCollision) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.36252 -0.07729 0.00388 0.06376 0.23788 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.576e-03 1.939e-04 8.131 9.07e-08 *** ## Vehicle_UseDriveLong 1.206e-03 2.750e-04 4.388 0.000284 *** ## Vehicle_UseDriveShort 1.752e-03 3.760e-04 4.659 0.000151 *** ## Vehicle_UsePleasure 2.096e-03 2.671e-04 7.847 1.57e-07 *** ## AgeB 7.881e-04 2.954e-04 2.668 0.014762 * ## AgeC 8.927e-04 3.411e-04 2.617 0.016503 * ## AgeD 9.567e-04 3.660e-04 2.614 0.016604 * ## AgeE 2.040e-03 4.331e-04 4.710 0.000134 *** ## AgeF 1.381e-03 5.526e-04 2.500 0.021237 * ## AgeG 1.353e-03 4.600e-04 2.942 0.008068 ** ## AgeH 1.395e-03 3.902e-04 3.575 0.001894 ** ## Claim_Count -8.694e-08 9.419e-07 -0.092 0.927375 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Gamma family taken to be 0.02045281) ## ## Null deviance: 3.20647 on 31 degrees of freedom ## Residual deviance: 0.42585 on 20 degrees of freedom ## AIC: 335.24 ## ## Number of Fisher Scoring iterations: 4 r_squared = 1 - ( fit$deviance / fit$df.null ) # psuedo r2 for GLMs r_squared ## [1] 0.9862631 # this model explains much more variance now that the error distribution has been specified correctly Probability distributions from the exponential family Claim Counts: Multiplicative Poisson model forms fit due to the poisson distribution is invariant to meatures of time. Frequency: Multiplicative Poisson model forms fit due to the poisson distribution is invariant to meatures of time. Severity: Multiplicative Gamma model forms fit because the gamma distribution is invariant to measures of currency. Retension and New Business: Binomial with logit model form fits becasue the binomial distribution is invariant to measures of success or failure. 6.3 GBM Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function. Source: Wikipedia Parameter tuning is prudent in machine learning! Examples: library(insuranceData) data(&quot;AutoCollision&quot;) library(gbm) # let&#39;s build a a GBM model which combines some weak learners into a strong learner as to boost the predictive power of those variables which contribute the most to the model fit &lt;- gbm(Claim_Count ~ Vehicle_Use + Age + Severity, data=AutoCollision, distribution = &quot;poisson&quot;, n.trees = 50, bag.fraction = 0.8) summary(fit) ## var rel.inf ## Vehicle_Use Vehicle_Use 47.60538 ## Age Age 40.59531 ## Severity Severity 11.79931 6.4 Ensemble learning In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone. Source: Wikipedia h2o / stacking 6.5 Additional Machine Learning Techniques xgboost: Extreme Gradient Boosting, which is an efficient implementation of gradient boosting framework. This package is its R interface. The package includes efficient linear model solver and tree learning algorithms. The package can automatically do parallel computation on a single machine which could be more than 10 times faster than existing gradient boosting packages. It supports various objective functions, including regression, classification and ranking. The package is made to be extensible, so that users are also allowed to define their own objectives easily. TDboost: A boosted Tweedie compound Poisson model using the gradient boosting. It is capable of fitting a flexible nonlinear Tweedie compound Poisson model (or a gamma model) and capturing interactions among predictors. glmnet: lasso, ridge, elasticnet: Extremely efficient procedures for fitting the entire lasso (least absolute shrinkage and selection operator) or elastic-net regularization path for linear regression, logistic and multinomial regression models, Poisson regression and the Cox model. Two recent additions are the multiple-response Gaussian, and the grouped multinomial. The algorithm uses cyclical coordinate descent in a path-wise fashion. randomForest: Classification and regression based on a forest of trees using random inputs. K-means / K-mediods: K Means Clustering is an unsupervised learning algorithm that tries to cluster data based on their similarity. Available in the base stats package "],
["communicate.html", "Chapter 7 Communicate 7.1 Tools for Communication and Reproducible Research", " Chapter 7 Communicate In Data Science our role is to be translators. We are tasked with translating buisness-driven inquires to discovering implicit knowledge from data and transforming knowledge into actionable results. 7.1 Tools for Communication and Reproducible Research knitr: Provides a general-purpose tool for dynamic report generation in R using Literate Programming techniques. rmarkdown: Convert R Markdown documents into a variety of formats (HTML, Word, PDF, Notebooks, Presentaions, dashboards). Bookdown (meta): Output formats and utilities for authoring books with R Markdown. shiny: Makes it incredibly easy to build interactive web applications with R. Automatic “reactive” binding between inputs and outputs and extensive pre-built widgets make it possible to build beautiful, responsive, and powerful applications with minimal effort. flexdashboard: Format for converting an R Markdown document to a grid oriented dashboard. The dashboard flexibly adapts the size of it’s components to the containing web page. "],
["references.html", "Chapter 8 References 8.1 R Resources 8.2 Financial Technology Resources", " Chapter 8 References 8.1 R Resources R for Data Science bookdown: Authoring Books with R Markdown 8.2 Financial Technology Resources Wharton FinTech #fintech "]
]
